{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675f8fee",
   "metadata": {},
   "source": [
    "# Capítulo 7. Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b20c62",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">0. Preparar el libro de trabajo </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac03df12-ec06-4764-a2b5-40f184dc6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importaciones comunes\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5e3b1b-79a5-492b-95e3-686dc2d99738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar TF y Keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfe8bc5-25b9-4966-ab57-bce525de103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparar Matplotlib\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a466e530-ac10-41b1-bb71-02ad06fe666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semillas a 42\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0df0182-8c2c-4052-b292-f5bfa6f6e60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "#Print de versión\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31323f-5a9d-4c54-aab6-ecc9b938911d",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">1. Cargar el set de Datos e importar el dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231f2d47-c1b4-40b4-9da7-0a39e0099948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga los datos a tu archivo de texto\n",
    "with open(\"quijote.txt\") as f:\n",
    "    quijote_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0f3875-85c6-47ca-b8f7-a7c075382b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
      "tiempo que vivía un hidalg\n"
     ]
    }
   ],
   "source": [
    "#Imprime algún texto aleatorio del archivo (como el 147)\n",
    "print(quijote_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1de44f9-4adc-4e2f-b721-7741a8eea9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.01234567:;?]abcdefghijlmnopqrstuvwxyz¡«»¿àáéíïñóùúü'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos viendo todos los caracters de \"minúsculas\" que trae el archivo\n",
    "\"\".join(sorted(set(quijote_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440228ef-4094-4b2a-bed2-fe89c3ea8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a usar la capa de TextVectorization para mapear el quijote a caracteres\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split = \"character\", standardize = \"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93768153-cb35-4f60-819b-3c2c83ef7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer.adapt([quijote_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80c038c3-eae8-4440-a0b8-7f44959f356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = text_vec_layer([quijote_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39a6a92-0553-4be4-bbf9-4dd6f0a4533e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([ 3,  7,  2, 11,  7,  2,  9, 11, 24,  4,  8,  2, 10,  3,  2,  9,  4,\n",
       "        2, 15,  4,  7, 14, 22,  4, 16,  2, 10,  3,  2, 14, 11, 20,  5,  2,\n",
       "        7,  5, 15, 21,  8,  3,  2,  7,  5,  2, 19, 11, 12,  3,  8,  5,  2,\n",
       "        4, 14,  5,  8, 10,  4,  8, 15,  3, 16,  2,  7,  5,  2, 22,  4,  2,\n",
       "       15, 11, 14, 22,  5, 17, 13, 12,  3, 15, 18,  5,  2, 19, 11,  3,  2,\n",
       "       23, 12, 23, 25,  4,  2, 11,  7,  2, 22, 12, 10,  4,  9, 24],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computa el vocabulario de términos de String sobre el quijote\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3408d99c-e0fa-4894-a8bd-5239e5672bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abandona los tokens 0 y 1 (padding), no los usaremos\n",
    "encoded -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d963cc82-05bc-4ba4-8bba-7dbf2800317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número total de caracteres distintos\n",
    "n_tokens = text_vec_layer.vocabulary_size()-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bff503d-8c8e-4158-8851-41235a64be7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f33943a-cf85-41b4-8339-3669fcf8a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2071088"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Número total de caracteres en total\n",
    "dataset_size = len(encoded)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89b3a9-d5e2-4ac2-8a7e-6b00e3c9009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usemos la clase de tokenizer de keras para codificar cada caracter como entero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f6b8c-330b-467d-8686-6f25f3c648ec",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">2. Separar el set en entrenamientos, pruebas y validación</span> ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c895cf80-8ffd-4904-bd2c-11a6f537a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crea una función que va a convertir el quijote en un dataset barajeado\n",
    "def to_dataset(sequence, length, shuffle = False, seed = None, batch_size = 32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift = 1, drop_remainder = True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed = seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:,:-1],window[:,1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4860e52a-53f9-416e-90fd-83cadbed5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a dividir el dataset en Train, valid y test usando la función anterior\n",
    "length = 100\n",
    "train_set = to_dataset(encoded[:1_000_000], length = length, shuffle = True, seed = 42 )\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length = length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16c7d29e-7e21-4400-a0ca-ccf590ac12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un modelo sencillo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = n_tokens, output_dim = 16),\n",
    "    tf.keras.layers.GRU(128, return_sequences = True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fd1e3-d9e4-4836-95f7-3ff858bb95e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3b241df-0057-4433-940e-0719eb700cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilamos \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a673b24-074b-482e-b2fa-ded2d6fc4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado que el modelo es tan grande, vamos a necesitar checkpoints\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"quijote_modelo\", monitor=\"val_accuracy\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ea9ed-4a67-4fdc-a25d-c434fd36645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma el checkpoint para guardar el modelo del quijote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e8b13-3b33-4ea3-899e-ce2adc0bfdf6",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">3. Construir y Entrenar el modelo Char-RNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9108d6-4b16-49e6-a4b8-971b7adef276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos el modelo. Nota que se puede tomar hasta 10 minutos por epoca, tal vez más\n",
    "history = model.fit(train_set, validation_data = valid_set, epochs = 10, callbacks = [model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94ebe5a0-9c75-41f5-8e7e-6d014ff2c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para cargar el modelo ya entrenado (por si no lo quisiste entrenar)\n",
    "model = tf.keras.models.load_model(\"quijote_modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40687604-f4f7-4f34-ac30-b6e3d596d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código para armar el modelo final\n",
    "quijote_modelo = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2),\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e604f-2653-45ab-9a9d-453bb5558f95",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">4. Generando Texto Falso</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "501f1d01-ab76-4b77-8dec-abb8bae4e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una función llamada next char para crear texto nuevo \n",
    "def next_char(text, temperature = 1):\n",
    "    y_proba = quijote_modelo.predict([text])[0,-1:]\n",
    "    rescaled_logits = tf.math.log(y_proba)/temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples = 1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "867c3888-b8fd-452d-b79f-d6440bec50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una función llamada extend_text para crear la secuencia completa\n",
    "def extend_text(text, n_chars = 50, temperature = 1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b1052d2-3d4d-4c60-9461-d2ed69e2fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambia la semilla a 42\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e238338f-d1f2-49a4-8380-6f3b3fad67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "La virtud más es perseguida de los malos de la mano de alguna de mi casa de la mano de alg\n"
     ]
    }
   ],
   "source": [
    "#Probamos nuestra función nueva\n",
    "print(extend_text(\"La virtud más es perseguida de los malos\", temperature = 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73b03b7f-4f4a-455d-b14f-636a1fff5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "La virtud más es perseguida de los malos hijos incledo el dovea ser su ignore, y no deje e\n"
     ]
    }
   ],
   "source": [
    "#Prueba 2\n",
    "print(extend_text(\"La virtud más es perseguida de los malos\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fc71c47-bb3c-4e80-bb5c-d9e7b2303862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "La virtud más es perseguida de los malosgeóÍÁ,3óütó-4',úéh,;4)íÉ2\"6v(ósá-!c¿üw.md?!.z\"à?](\n"
     ]
    }
   ],
   "source": [
    "#Prueba 3\n",
    "print(extend_text(\"La virtud más es perseguida de los malos\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58472d-6a9e-4ddc-bec0-58d1c7b249d6",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">5. Stateful RNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "734b47e7-b7db-4d19-860d-c0d85ea79581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos definiendo una función que prepara el dataset para nuestro stateful RNN\n",
    "def to_dataset_for_stateful_rnn(sequence,length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d368933d-ac98-461b-91c3-a7c7a8ee4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora separamos en train, valid y test\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[1_000_000:], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_600_000], length)\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3eb04013-5df4-4661-a0cd-30a6efcd7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK sigue aplicar el modelo secuencial en keras - embedding, GRU, Dense\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f89a1e90-3c7b-42e6-8ce3-063fd0f91fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al final de cada epoca debemos resetear los estados antes de vovler al inicio del texto\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5769ac29-7113-4695-bb68-a6d2c2963293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usa una celda diferente para guardar los checkpoints\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"qujote_stateful_modelo\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d58a488-761b-4a1c-a7b5-9b0b82ea5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y ahora compilamos y Ejecutamos el modelo\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322f891-6fc6-46f3-80a6-fae159456719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y ahora compilamos y ajustamos el modelo\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
    "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def0f5c-f460-43b9-aa00-1d7f1eaf4183",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">6. Sentiment Analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04c1b62e-a7aa-480e-85a0-8c02d6bdbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa Tensroflow Datasetse como tfds\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af7c6c78-aec7-46ac-8000-eb7c613afabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga los datos de train, valid y test\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name = \"imdb_reviews\",\n",
    "    split =[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d850913b-bf17-43be-95fe-1862b31acdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma el dataset de train - barajea con buffer de 5000, bachealo con 32 y prefecth\n",
    "train_set = raw_train_set.shuffle(5000, seed = 42).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06439b11-1259-4d54-b68f-fd81b9bd26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validación y test con batch de 32 y prefetch\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4912321-2f59-4a6b-bcbf-acd085c6e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "label 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "label 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
      "label 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
      "label 1\n"
     ]
    }
   ],
   "source": [
    "#Vamos viendo unos reviews de muestra - arma un loop que agarre 4 reviews y las decodifique\n",
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\"))\n",
    "    print(\"label\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a50bbac9-66ab-4d33-b223-4c2b340e97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 1000 como tamaño de vocabulario\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "432ee088-12f7-4a5c-9daa-2b20a0a6c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma tu capa de textVectorization\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d99fbb39-2245-4d62-949e-05e978692c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica tu .adapt\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6be85601-b010-4468-beb6-5dff6c0128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma el modelo, capa de text_vec_layer, embedding, GRU y dense, embedding de 128\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, 128),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b16b9063-e9ad-4209-833f-c798222eeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compila con crossentropy binaria y nadam\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4ff9a19-b215-42f5-a813-c30f8d860d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "704/704 [==============================] - 28s 35ms/step - loss: 0.6934 - accuracy: 0.5039 - val_loss: 0.6936 - val_accuracy: 0.4992\n",
      "Epoch 2/2\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.6926 - accuracy: 0.5011 - val_loss: 0.6941 - val_accuracy: 0.5004\n"
     ]
    }
   ],
   "source": [
    "#Ejecuta tu modelo\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4a9a70d-4a98-45f1-8526-04a613655c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6942 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6941766142845154, 0.5]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evalúa tu modelo\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc9354f0-23da-41f7-8151-b7a2be3a2883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.45218506],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875],\n",
       "       [0.51518875]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predice un test set\n",
    "model.predict(test_set.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fcccae3-ad95-407e-b5f3-0cab5cfeaf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#Checa los resultados que si eran de verad\n",
    "for review,label in test_set.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390b176-6adc-4e8e-853d-25dbcf18873d",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">6.1 Masking</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7da9d46d-64df-42d5-ab60-842b48b8d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma tu modelo con Masking\n",
    "embed_size=128\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero = True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2daef23-db09-4b02-a9c2-c65a7ad7f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilalo\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b0b1896-97d3-419e-ba0d-4f74c9ced7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 33s 39ms/step - loss: 0.5825 - accuracy: 0.6879 - val_loss: 0.6501 - val_accuracy: 0.6860\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.3666 - accuracy: 0.8420 - val_loss: 0.3789 - val_accuracy: 0.8268\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.3187 - accuracy: 0.8646 - val_loss: 0.3264 - val_accuracy: 0.8548\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 26s 36ms/step - loss: 0.2986 - accuracy: 0.8752 - val_loss: 0.3288 - val_accuracy: 0.8568\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 25s 36ms/step - loss: 0.2811 - accuracy: 0.8804 - val_loss: 0.3370 - val_accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "#Ajustalo\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0016c2c-2b98-46f7-a101-2869c4715428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3536 - accuracy: 0.8510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3535629212856293, 0.8510400056838989]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evalualo\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b2bbac74-272a-4846-abd1-df9545267bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.92258024],\n",
       "       [0.978152  ],\n",
       "       [0.05177713],\n",
       "       [0.37396833],\n",
       "       [0.9917114 ],\n",
       "       [0.992811  ],\n",
       "       [0.99797446],\n",
       "       [0.9949511 ],\n",
       "       [0.8283124 ],\n",
       "       [0.9814276 ],\n",
       "       [0.01334537],\n",
       "       [0.64912254],\n",
       "       [0.8617164 ],\n",
       "       [0.14778893],\n",
       "       [0.77455235],\n",
       "       [0.8525385 ],\n",
       "       [0.9904516 ],\n",
       "       [0.41010854],\n",
       "       [0.9301605 ],\n",
       "       [0.00269966],\n",
       "       [0.0019085 ],\n",
       "       [0.98036754],\n",
       "       [0.02791318],\n",
       "       [0.72845066],\n",
       "       [0.99120444],\n",
       "       [0.30135474],\n",
       "       [0.07820202],\n",
       "       [0.516478  ],\n",
       "       [0.26912013],\n",
       "       [0.9400048 ],\n",
       "       [0.8926644 ],\n",
       "       [0.9854929 ]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict al primer bache vs\n",
    "model.predict(test_set.take(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1f094bb-55fb-4e1c-b2a6-e19511ae5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#La realidad de los labels\n",
    "for review,label in test_set.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89059924-fe47-417d-aea1-fd59b8f09c77",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">7. Encoder - Decoder para Traducción</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "098758e3-8880-4343-a308-86b671e1b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa IO y lee el texto de traducciones inglés y español\n",
    "import io\n",
    "read_file= io.open(\"spa.txt\", \"r\", encoding = \"utf8\")\n",
    "text = read_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4747bf65-45d3-4e68-8cbc-7afb130b9a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.\\tVe.\\nGo.\\tVete.\\nGo.\\tVaya.\\nGo.\\tVáyase.\\nHi.\\tHola.\\nRun!\\t¡Corre!\\nRun.\\tCorred.\\nWho?\\t¿Quién?\\nFire!\\t¡Fueg'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5dd02e24-a948-4449-8e3f-72d708040737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traite numpy y quita los caracteres superfluos en español. Arma las parejas y barajealas. Separalas en oraciones en, oraciones es\n",
    "import numpy as np\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\",\"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d6e158a-506e-4b3d-8db8-2eb78f47be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "#Imprime 3 oraciones de inglés/español\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4f581b8a-f6f4-4a7d-aa25-fb5cfa57639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocab Size de 1000 y max length de 50\n",
    "vocab_size = 1000\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3204071f-a932-4dda-a9b0-6761c4eaf0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte ingles a números\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e882f76-69a5-42dc-a222-4ada29424f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte Español a números\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d4630ba-2934-4aee-961e-766ddbd29836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usa Adapt para computar un vocabulario de strings desde los tokens en tu vocabulario vectorizado (se tarda)\n",
    "text_vec_layer_en.adapt(sentences_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22bb8763-e93f-4b82-8732-b0e9d1e4ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_es.adapt(([f\"startofseq {s} endofseq\" for s in sentences_es]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99376c61-d1b9-447b-91d1-0cd770380c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regresa los 10 tokens mas usados de la capa de ingles\n",
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e3b557b-5438-4bbe-8054-bf1803f941ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regresa los 10 tokens mas usados de la capa de español\n",
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b222e2a-aecb-4d01-8371-d8856096ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define tus sets de train, validación, train y validación para el decoder, y las Ys\n",
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76e1eed4-739a-403e-bd1e-3a8285d06a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define las capas de entradas del encoder y del decoder\n",
    "tf.random.set_seed(42)\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype = tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86314c27-91d9-4ac4-ac8b-e269160b2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora asignale tus textos vectorizados a esas capas nuevas\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60f8391f-9da9-4087-9d55-0bae63a50a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepara una capa de embedding cada uno(decoder y encoder)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero = True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "80ca8fa1-dd2e-41a0-ae52-743c6cd08266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y ahora mete tus input_ids a la capa de embedding\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "04feddfe-843d-495c-a3b5-6e102c00ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define el encoder como 1 sola capa LSTM de 512 neuronas, pasa tus embeddings por esa capa ye scupe los outputs y el state\n",
    "encoder = tf.keras.layers.LSTM(512, return_state = True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d07a8e53-daac-4336-b526-a637c6e654b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repite el proceso correspondiente para el decoder\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "97154a82-e9c3-4777-85ae-fa4b5633800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La capa de salida será una capa Densa del mismo tamaño que el vocabulario con activacion softmax, nos escupira la probabilidad Y de alguna palabra tomando los decoder outputs como entrada\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c9fcd64-010d-404d-9847-69cc75349a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arma tu modelo\n",
    "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs], outputs = [Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d869b8f9-e171-4738-8815-206f9c0ef7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compila\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d0de14d8-3fcc-44ed-8743-0fccb530cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma un checkpoint para guardar el modelo\n",
    "tmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"traductor_modelo\", monitor=\"val_accuracy\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a665ee26-17f5-4a32-b1a6-266660f40efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.4187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 151s 45ms/step - loss: 0.4213 - accuracy: 0.4187 - val_loss: 0.3174 - val_accuracy: 0.5106\n",
      "Epoch 2/10\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.5631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 131s 42ms/step - loss: 0.2726 - accuracy: 0.5631 - val_loss: 0.2442 - val_accuracy: 0.5957\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.6356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 137s 44ms/step - loss: 0.2137 - accuracy: 0.6356 - val_loss: 0.2112 - val_accuracy: 0.6433\n",
      "Epoch 4/10\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.6826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 138s 44ms/step - loss: 0.1796 - accuracy: 0.6826 - val_loss: 0.1961 - val_accuracy: 0.6635\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.7162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 134s 43ms/step - loss: 0.1558 - accuracy: 0.7162 - val_loss: 0.1895 - val_accuracy: 0.6725\n",
      "Epoch 6/10\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.7448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 134s 43ms/step - loss: 0.1365 - accuracy: 0.7448 - val_loss: 0.1873 - val_accuracy: 0.6755\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.7696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: traductor_modelo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 133s 42ms/step - loss: 0.1202 - accuracy: 0.7696 - val_loss: 0.1883 - val_accuracy: 0.6799\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 90s 29ms/step - loss: 0.1058 - accuracy: 0.7926 - val_loss: 0.1916 - val_accuracy: 0.6795\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 93s 30ms/step - loss: 0.0932 - accuracy: 0.8142 - val_loss: 0.1970 - val_accuracy: 0.6762\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 96s 31ms/step - loss: 0.0823 - accuracy: 0.8328 - val_loss: 0.2025 - val_accuracy: 0.6739\n"
     ]
    }
   ],
   "source": [
    "#Ejecuta a 10 epocas, nota que tienes que meter 2 sets de entrenamiento y validación para X (enc y dec) - se va a tardar como 20 minutos por epoca sin GPU! (1 min sin gpu)\n",
    "history=model.fit((X_train, X_train_dec), Y_train, epochs = 10, validation_data=((X_valid, X_valid_dec), Y_valid), callbacks = [tmodel_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "69689e6a-abb1-4c7b-944a-1a34d9c5ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ahora a armar el traductor\n",
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])\n",
    "        X_dec = np.array([\"startofseq\" + translation])\n",
    "        y_proba = model.predict((X, X_dec))[0,word_idx]\n",
    "        predicted_word_id=np.argmax(y_proba)\n",
    "        predicted_word =text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5a5fa37f-ada9-4dbd-bae1-149af5b1a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prueba con i like soccer\n",
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "841564b5-ca95-4ff5-8653-e4349ba227ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[UNK] el fútbol y también ir a la playa'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prueba con una oración larga como i like soccer and going to the beach\n",
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62134c-9943-4073-98a6-de12dc63e61a",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">7.1 Capas Recurrentes Bidireccionales</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb39f9-ddd3-4f98-b482-c0ce35c118f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arma un modelo secuencial con cuna capa GRU y una capa GRU bidireccional\n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2ee8a-07e1-4825-9ed1-979133b304e9",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">8. Atención</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0718a589-4611-4aef-b962-4bd5fa3a09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Envuelve tu encoder en una capa bidireccional, como hace ratito\n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences = True, return_state = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0b1d8587-a512-4004-a290-4b77a246d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a volver a armar esta parte del modelo, casi igualita a la anterior, para que cache nuestra nueva onda bidireccional\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),tf.concat(encoder_state[1::2], axis=-1)]\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "ecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "61aa3d4f-362c-4994-897f-473e9bb55412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos las capas de atención en keras\n",
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7be09bf8-f2f9-49f7-9a14-2896a9a6bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y terminamos el modelo\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a4a8f68b-f067-4e28-968b-f496d2e6c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3569aa07-2509-4427-a830-008a2f6e859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 181s 51ms/step - loss: 0.3864 - accuracy: 0.4601 - val_loss: 0.2595 - val_accuracy: 0.5917\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 155s 50ms/step - loss: 0.2199 - accuracy: 0.6433 - val_loss: 0.2076 - val_accuracy: 0.6590\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 150s 48ms/step - loss: 0.1820 - accuracy: 0.6929 - val_loss: 0.1919 - val_accuracy: 0.6823\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 150s 48ms/step - loss: 0.1614 - accuracy: 0.7207 - val_loss: 0.1853 - val_accuracy: 0.6924\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 151s 48ms/step - loss: 0.1459 - accuracy: 0.7422 - val_loss: 0.1835 - val_accuracy: 0.6967\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 151s 48ms/step - loss: 0.1334 - accuracy: 0.7611 - val_loss: 0.1840 - val_accuracy: 0.6983\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 150s 48ms/step - loss: 0.1232 - accuracy: 0.7758 - val_loss: 0.1849 - val_accuracy: 0.7020\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 149s 48ms/step - loss: 0.1145 - accuracy: 0.7894 - val_loss: 0.1881 - val_accuracy: 0.7018\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 148s 47ms/step - loss: 0.1070 - accuracy: 0.8010 - val_loss: 0.1910 - val_accuracy: 0.7013\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 153s 49ms/step - loss: 0.1012 - accuracy: 0.8097 - val_loss: 0.1938 - val_accuracy: 0.7022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebc13b49d0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
